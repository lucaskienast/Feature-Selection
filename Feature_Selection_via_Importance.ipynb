{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature Selection via Importance.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JianC7UgQajX"
      },
      "source": [
        "# Feature Selection with Importance\n",
        "\n",
        "Feature importance scores can be used to help interpret the data, but they can also be used directly to help rank and select features that are most useful to a predictive model.\n",
        "\n",
        "We can demonstrate this with a small example. Our synthetic dataset has 1,000 examples each with 10 input variables, five of which are redundant and five of which are important to the outcome. We can use feature importance scores to help select the five variables that are relevant and only use them as inputs to a predictive model. First, we can split the training dataset into train and test sets and train a model on the training dataset, make predictions on the test set and evaluate the result using classification accuracy. We will use a logistic regression model as the predictive model. This provides a baseline for comparison when we remove some features using feature importance scores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZZhtRFhQ20P"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZkGNFuPQZ0C"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEZxZg8HREJC"
      },
      "source": [
        "## Baseline model (logistic regression)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0hdX5agRByi",
        "outputId": "d06649bf-b267-4997-d33e-b3c532bf137e"
      },
      "source": [
        "# define the dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 10) (1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iouABhEpRK4W"
      },
      "source": [
        "# split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Box4bFIQRNLx",
        "outputId": "abf0a205-9b27-44fd-b308-49d99ecf2e10"
      },
      "source": [
        "# fit the model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xfWspEHROdp"
      },
      "source": [
        "# evaluate the model\n",
        "yhat = model.predict(X_test)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4nnCu2TRP8H",
        "outputId": "a3b36240-e6a8-4d42-a115-d19fad9ae92e"
      },
      "source": [
        "# evaluate predictions\n",
        "accuracy = accuracy_score(y_test, yhat)\n",
        "print('Accuracy: %.2f' % (accuracy*100))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 84.55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYbRx2F5RSCm"
      },
      "source": [
        "Running the example first the logistic regression model on the training dataset and evaluates it on the test set. In this case we can see that the model achieved the classification accuracy of about 84.55 percent using all features in the dataset.\n",
        "\n",
        "Given that we created the dataset, we would expect better or the same results with half the number of input variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPKDJBRiRiRX"
      },
      "source": [
        "## Feature Selection via Importance\n",
        "\n",
        "We could use any of the various possible feature importance scores, but in this case we will use the feature importance scores provided by random forest. We can use the SelectFromModel class to define both the model we wish to calculate importance scores, RandomForestClassifier in this case, and the number of features to select, 5 in this case.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQlORskbRQSZ"
      },
      "source": [
        "# feature selection\n",
        "def select_features(X_train, y_train, X_test):\n",
        "\t# configure to select a subset of features\n",
        "\tfs = SelectFromModel(RandomForestClassifier(n_estimators=1000), max_features=5)\n",
        "\t# learn relationship from training data\n",
        "\tfs.fit(X_train, y_train)\n",
        "\t# transform train input data\n",
        "\tX_train_fs = fs.transform(X_train)\n",
        "\t# transform test input data\n",
        "\tX_test_fs = fs.transform(X_test)\n",
        "\treturn X_train_fs, X_test_fs, fs"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCalLrClR9m0"
      },
      "source": [
        "We can fit the feature selection method on the training dataset.\n",
        "\n",
        "This will calculate the importance scores that can be used to rank all input features. We can then apply the method as a transform to select a subset of 5 most important features from the dataset. This transform will be applied to the training dataset and the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd7mF2a6SAmv"
      },
      "source": [
        "# feature selection\n",
        "X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1Y5hyODSEdX",
        "outputId": "7eac9db1-92f9-4a5d-cdb0-e7afd72472ba"
      },
      "source": [
        "# fit the model\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train_fs, y_train)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdRFxXM0SG1M"
      },
      "source": [
        "# evaluate the model\n",
        "yhat = model.predict(X_test_fs)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IvjdecQSIS3",
        "outputId": "4a699fc5-5894-48a0-9d20-42b3feacbe38"
      },
      "source": [
        "# evaluate predictions\n",
        "accuracy = accuracy_score(y_test, yhat)\n",
        "print('Accuracy: %.2f' % (accuracy*100))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 84.55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa27iiPMSKyx"
      },
      "source": [
        "Running the example first performs feature selection on the dataset, then fits and evaluates the logistic regression model as before. In this case, we can see that the model achieves the same performance on the dataset, although with half the number of input features. As expected, the feature importance scores calculated by random forest allowed us to accurately rank the input features and delete those that were not relevant to the target variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RM9gKQROSImu"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}